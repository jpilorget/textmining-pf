{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba de clasificador LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import copy\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "use_plot = True\n",
    "use_save = True\n",
    "if use_save:\n",
    "    import pickle\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, label_size, batch_size, use_gpu):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.use_gpu = use_gpu\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.hidden2label = nn.Linear(hidden_dim, label_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        if self.use_gpu:\n",
    "            h0 = Variable(torch.zeros(1, self.batch_size, self.hidden_dim).cuda())\n",
    "            c0 = Variable(torch.zeros(1, self.batch_size, self.hidden_dim).cuda())\n",
    "        else:\n",
    "            h0 = Variable(torch.zeros(1, self.batch_size, self.hidden_dim))\n",
    "            c0 = Variable(torch.zeros(1, self.batch_size, self.hidden_dim))\n",
    "        return (h0, c0)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        x = embeds.view(len(sentence), self.batch_size, -1)\n",
    "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
    "        y  = self.hidden2label(lstm_out[-1])\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, DATA_DIR, filenames):\n",
    "        self.dictionary = Dictionary()\n",
    "        self.data = self.tokenize(DATA_DIR, filenames)\n",
    "\n",
    "    def tokenize(self, DATA_DIR, filenames):\n",
    "        for filename in filenames:\n",
    "            path = os.path.join(DATA_DIR, filename)\n",
    "            with open(path, 'r') as f:\n",
    "                tokens = 0\n",
    "                for line in f:\n",
    "                    words = line.split() + ['<eos>']\n",
    "                    tokens += len(words)\n",
    "                    for word in words:\n",
    "                        self.dictionary.add_word(word)\n",
    "\n",
    "            # Tokenize file content\n",
    "            with open(path, 'r') as f:\n",
    "                ids = torch.LongTensor(tokens)\n",
    "                token = 0\n",
    "                for line in f:\n",
    "                    words = line.split() + ['<eos>']\n",
    "                    for word in words:\n",
    "                        ids[token] = self.dictionary.word2idx[word]\n",
    "                        token += 1\n",
    "\n",
    "        return ids\n",
    "\n",
    "class TxtDatasetProcessing(Dataset):\n",
    "    def __init__(self, data_path, txt_path, txt_filename, label_filename, sen_len, corpus):\n",
    "        self.txt_path = os.path.join(data_path, txt_path)\n",
    "        # reading txt file from file\n",
    "        txt_filepath = os.path.join(data_path, txt_filename)\n",
    "        fp = open(txt_filepath, 'r')\n",
    "        self.txt_filename = [x.strip() for x in fp]\n",
    "        fp.close()\n",
    "        # reading labels from file\n",
    "        label_filepath = os.path.join(data_path, label_filename)\n",
    "        fp_label = open(label_filepath, 'r')\n",
    "        labels = [int(x.strip()) for x in fp_label]\n",
    "        fp_label.close()\n",
    "        self.label = labels\n",
    "        self.corpus = corpus\n",
    "        self.sen_len = sen_len\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        filename = os.path.join(self.txt_path, self.txt_filename[index])\n",
    "        fp = open(filename, 'r')\n",
    "        txt = torch.LongTensor(np.zeros(self.sen_len, dtype=np.int64))\n",
    "        count = 0\n",
    "        clip = False\n",
    "        for words in fp:\n",
    "            for word in words.split():\n",
    "                if word.strip() in self.corpus.dictionary.word2idx:\n",
    "                    if count > self.sen_len - 1:\n",
    "                        clip = True\n",
    "                        break\n",
    "                    txt[count] = self.corpus.dictionary.word2idx[word.strip()]\n",
    "                    count += 1\n",
    "            if clip: break\n",
    "        label = torch.LongTensor([self.label[index]])\n",
    "        return txt, label\n",
    "    def __len__(self):\n",
    "        return len(self.txt_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data\\\\train_txt.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-295554e78789>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mtrain_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTRAIN_FILE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mtest_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTEST_FILE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0mfp_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[0mtrain_filenames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTRAIN_DIR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfp_train\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mfilenames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_filenames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data\\\\train_txt.txt'"
     ]
    }
   ],
   "source": [
    "DATA_DIR = 'data'\n",
    "TRAIN_DIR = 'train_txt'\n",
    "TEST_DIR = 'test_txt'\n",
    "TRAIN_FILE = 'train_txt.txt'\n",
    "TEST_FILE = 'test_txt.txt'\n",
    "TRAIN_LABEL = 'train_label.txt'\n",
    "TEST_LABEL = 'test_label.txt'\n",
    "\n",
    "## parameter setting\n",
    "epochs = 50\n",
    "batch_size = 5\n",
    "use_gpu = torch.cuda.is_available()\n",
    "learning_rate = 0.01\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    lr = learning_rate * (0.1 ** (epoch // 10))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return optimizer\n",
    "\n",
    "if __name__=='__main__':\n",
    "    ### parameter setting\n",
    "    embedding_dim = 100\n",
    "    hidden_dim = 50\n",
    "    sentence_len = 32\n",
    "    train_file = os.path.join(DATA_DIR, TRAIN_FILE)\n",
    "    test_file = os.path.join(DATA_DIR, TEST_FILE)\n",
    "    fp_train = open(train_file, 'r')\n",
    "    train_filenames = [os.path.join(TRAIN_DIR, line.strip()) for line in fp_train]\n",
    "    filenames = copy.deepcopy(train_filenames)\n",
    "    fp_train.close()\n",
    "    fp_test = open(test_file, 'r')\n",
    "    test_filenames = [os.path.join(TEST_DIR, line.strip()) for line in fp_test]\n",
    "    fp_test.close()\n",
    "    filenames.extend(test_filenames)\n",
    "\n",
    "    corpus = DP.Corpus(DATA_DIR, filenames)\n",
    "nlabel = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ### create model\n",
    "    model = LSTMC.LSTMClassifier(embedding_dim=embedding_dim,hidden_dim=hidden_dim,\n",
    "                           vocab_size=len(corpus.dictionary),label_size=nlabel, batch_size=batch_size, use_gpu=use_gpu)\n",
    "    if use_gpu:\n",
    "        model = model.cuda()\n",
    "    ### data processing\n",
    "    dtrain_set = DP.TxtDatasetProcessing(DATA_DIR, TRAIN_DIR, TRAIN_FILE, TRAIN_LABEL, sentence_len, corpus)\n",
    "\n",
    "    train_loader = DataLoader(dtrain_set,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=4\n",
    "                         )\n",
    "    dtest_set = DP.TxtDatasetProcessing(DATA_DIR, TEST_DIR, TEST_FILE, TEST_LABEL, sentence_len, corpus)\n",
    "\n",
    "    test_loader = DataLoader(dtest_set,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=False,\n",
    "                          num_workers=4\n",
    "                         )\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    train_loss_ = []\n",
    "    test_loss_ = []\n",
    "    train_acc_ = []\n",
    "    test_acc_ = []\n",
    "    ### training procedure\n",
    "    for epoch in range(epochs):\n",
    "        optimizer = adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "        ## training epoch\n",
    "        total_acc = 0.0\n",
    "        total_loss = 0.0\n",
    "        total = 0.0\n",
    "        for iter, traindata in enumerate(train_loader):\n",
    "            train_inputs, train_labels = traindata\n",
    "            train_labels = torch.squeeze(train_labels)\n",
    "\n",
    "            if use_gpu:\n",
    "                train_inputs, train_labels = Variable(train_inputs.cuda()), train_labels.cuda()\n",
    "            else: train_inputs = Variable(train_inputs)\n",
    "\n",
    "            model.zero_grad()\n",
    "            model.batch_size = len(train_labels)\n",
    "            model.hidden = model.init_hidden()\n",
    "            output = model(train_inputs.t())\n",
    "\n",
    "            loss = loss_function(output, Variable(train_labels))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # calc training acc\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total_acc += (predicted == train_labels).sum()\n",
    "            total += len(train_labels)\n",
    "            total_loss += loss.data[0]\n",
    "\n",
    "        train_loss_.append(total_loss / total)\n",
    "        train_acc_.append(total_acc / total)\n",
    "        ## testing epoch\n",
    "        total_acc = 0.0\n",
    "        total_loss = 0.0\n",
    "        total = 0.0\n",
    "        for iter, testdata in enumerate(test_loader):\n",
    "            test_inputs, test_labels = testdata\n",
    "            test_labels = torch.squeeze(test_labels)\n",
    "\n",
    "            if use_gpu:\n",
    "                test_inputs, test_labels = Variable(test_inputs.cuda()), test_labels.cuda()\n",
    "            else: test_inputs = Variable(test_inputs)\n",
    "\n",
    "            model.batch_size = len(test_labels)\n",
    "            model.hidden = model.init_hidden()\n",
    "            output = model(test_inputs.t())\n",
    "\n",
    "            loss = loss_function(output, Variable(test_labels))\n",
    "\n",
    "            # calc testing acc\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total_acc += (predicted == test_labels).sum()\n",
    "            total += len(test_labels)\n",
    "            total_loss += loss.data[0]\n",
    "        test_loss_.append(total_loss / total)\n",
    "        test_acc_.append(total_acc / total)\n",
    "\n",
    "        print('[Epoch: %3d/%3d] Training Loss: %.3f, Testing Loss: %.3f, Training Acc: %.3f, Testing Acc: %.3f'\n",
    "              % (epoch, epochs, train_loss_[epoch], test_loss_[epoch], train_acc_[epoch], test_acc_[epoch]))\n",
    "\n",
    "    param = {}\n",
    "    param['lr'] = learning_rate\n",
    "    param['batch size'] = batch_size\n",
    "    param['embedding dim'] = embedding_dim\n",
    "    param['hidden dim'] = hidden_dim\n",
    "    param['sentence len'] = sentence_len\n",
    "\n",
    "    result = {}\n",
    "    result['train loss'] = train_loss_\n",
    "    result['test loss'] = test_loss_\n",
    "    result['train acc'] = train_acc_\n",
    "    result['test acc'] = test_acc_\n",
    "    result['param'] = param\n",
    "\n",
    "    if use_plot:\n",
    "        import PlotFigure as PF\n",
    "        PF.PlotFigure(result, use_save)\n",
    "    if use_save:\n",
    "        filename = 'log/LSTM_classifier_' + datetime.now().strftime(\"%d-%h-%m-%s\") + '.pkl'\n",
    "result['filename'] = filename"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
